{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOK9HZDgo/yz5AQn1tXbTvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shin-mat/Google-Colab/blob/main/distil_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git accelerate datasets[audio]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp1H4SEg7EWn",
        "outputId": "dea329a3-f65c-4284-87a3-7706ff52d66f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.2\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-4tuu89sq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-4tuu89sq\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 3cefac1d974db5e2825a0cb2b842883a628be7a0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets[audio]\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets[audio])\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets[audio])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (3.4.1)\n",
            "Collecting multiprocess (from datasets[audio])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets[audio]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (3.9.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.12.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.10.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (2023.11.17)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets[audio]) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[audio]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[audio]) (2023.3.post1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->datasets[audio]) (4.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets[audio]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.37.0.dev0-py3-none-any.whl size=8281628 sha256=9094ff5f41bcc9f565d25a505f3e9be6ca95759b0eb30eb8ecc745f8f8d54181\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-38ht8ib8/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, accelerate, transformers, datasets\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed accelerate-0.25.0 datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6 transformers-4.37.0.dev0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Add new tokens to the tokenizer\n",
        "new_tokens = [\"new_token\"]  # replace with your new tokens\n",
        "added_tokens = processor.tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "# Resize the token embeddings of the model\n",
        "model.resize_token_embeddings(len(processor.tokenizer))\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=30,\n",
        "    batch_size=16,\n",
        "    return_timestamps=True,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Path of input file\n",
        "input_file = \"A-F_conv.m4a\"\n",
        "# Path of output file\n",
        "output_file = \"A-F_conv.wav\"\n",
        "\n",
        "# Converting audio files with ffmpeg\n",
        "subprocess.run([\"ffmpeg\", \"-i\", input_file, output_file])\n",
        "\n",
        "result = pipe(output_file)  # Corrected this line\n",
        "print(result[\"text\"])\n",
        "with open('output.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xz6gWDu7NhQ",
        "outputId": "7000c7f7-5748-4cb4-fcbd-e94c799a4e92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I also replied I was so angry yeah and then like what kind of thing is that urgent that you need to email us twice all the time in the evening I messaged you in the morning you are busy yourself and then when you get time at 10 30 11 o'clock at night you start messaging and you expect answers yeah just like i i i talked the same thing to pervane it's like he paid someone for eight hours but then he expected more 14 hours it's not also about hours for now some work some things are more you know you can do your experiment for eight hours you might not be so tired but you know when you're doing these kind of things it's like not about hours it's more like how much your mind is getting exhausted yes right this kind of work for i think two hours also is very taxing yes and also like at least show some appreciation like no you just say one thank you and you think you're done with it everything I'm dead I know it's so annoying I totally no no no you're on 20th like two days are left today is already over and Monday is a holiday No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no to the iris we are aiming to some other Asian countries are very efficient not not in the US Fiona today we had the website meeting they literally haven't done anything and then again Dr. Wu is yelling at us wow why you guys should follow up you know I'm looking at him like this like what yeah you yell at the people that should work, not us. So, culture is different. Next week? Can we schedule a meeting whenever? Saya akan mengatur mesyuarat apabila Saya boleh mengatur mesyuarat dengan kebaikan anda Anda berjalan Teruskan I already it. Oh, okay. Can we schedule a meeting? When can we? Alright. Yeah. Okay. I'm going to go ahead and do it. I'm going to go ahead and do it. I'm going to go ahead and do it. I'm going to go ahead and do it. I'm when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be? when can be 오! 오! 이미 보내졌어? 보내줬어? 보내줬어? 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 안돼 Oh, it's already sent? What? Which one? MTA request? No, no, no. No. What? No. It is not. It's here. No, people preparing for the briefing. No. It's here, it's here, Xenia. Oh, drop, drop. Yeah. Oh my god. Me, me, god. That's why you should drop my email in a separate document. No, we did not send. I just clicked something and it just went down. We both got heart attack. Yeah, when I say something important, I usually keep a separate file. And then double check everything's written correctly. Yeah, that's okay nice this is okay so let me do one thing it's first just copy this yes copy okay any closer can you close? just this card which one? now let's go no now we'll have to make it again no what external institute? and no which one? no because it's here oh here oh so it didn't go no Shinya you were replying to this email right yes but no this email is not that former email it's saved in my don't worry we can paste it yes we'll have to make a new one where was the conversationこのイメージはあなたから送られたと思う。 아... 아... 아... 아... 아... 아... 아... 아... 아... 아... 아... 아... 아... 아... No, it's just wikileaks port. Reading port no, no. Where is the mail? Yeah, right? This one. No, no, no. Reading port. No. Okay, it's not. No. 아.. 이거..o.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아.. 이거.. 아 This is what you were writing, right? Fine. You got it. We got a heart attack. Win. Okay. Okay, yes. Send now before we delete or make a mistake again. hope you did not write his name as bird. It's fine. Okay. So it's okay. Send. So this guy, his name is Brit. Okay. So this guy, his name is Brit. Okay. So this guy, his name is Brit. Okay. So this guy, his name is Brit. Okay. So this guy, his name is Brit. Okay. So this guy, his name is Brit. Okay. So this guy, his name is Brit. Okay. So this not write his name as bird it's fine okay so it's a scent so this guy his name is breed something that's still a cute name Bird Shinia, first send Yeah, okay Just like, when I acknowledged my community, I was so Kimi Mae, it's a Sandra Mas- Mas-kita Mas-kita, yeah, I read it Sandra Mas-kita Yeah, so it's my community member mosquito mosquito yeah sandro mosquito so this is my community member he slasim looks so much like mosquito hahaha i thought you were saying mastura and i was like how do you pronounce it and then just try not to like... How's your document going? I mean, Houston side doesn't going. The OPT is still waiting. It says one week decision. Oh, it's waiting with USCIS? No, yeah, USCIS. USCIS. Yeah. Yeah, USCIS. USCIS. But anyway, I have OPT after I apply. I have a 60-day grace period. You can work. No, I can't work. It means I am legally staying in this. Yeah, you don't have to go back. Yes. But you can work at me? I can't. I will be locked out logged out yeah anyway like anyway why do i transfer to here so what's the last date that you can january 4th homeless labless not homeless yeah and uh and then you my daughter's I she also has a same issue she's like yeah so you know what I didn't know. I'm surprised because J2 literally minimum days on the website J2 EAD employment is 60 days. From UCIS side. Yeah, me too. OBT is the same thing. What are they thinking? doctor who already yelled me once like last weekend when i open the door he's like a coffee and then we are like it was very heavy enough to open the door he said why don't you apply that i warned you so much time i told you so many times why don't you do that and then uh i already did it i'm just waiting why don't you apply that i warned you so much time i told you so many times why don't you do that i'm like uh i already did that i'm just waiting why don't you do it early i'm like it's useless right now like when you say why don't you do it earlier it's there like oh even me still feel so stressful but yeah so i mean fiona i understand your part because you were already on a job, you were working, you were doing. But for Mastura's part, I don't understand why so much delay for her because she came here in September. We went, no, sorry, November. So she, I think Houston did not give her her appointment letter, it means. No. So, no. So she cannot have appointment letter until she has EAD so it's the reverse so she's so she did not apply for her EAD soon because if she had applied for her EAD maybe she's in a chaotic too yeah because I'm not saying like I'm not kind of pointing out the fault but I mean what Dr. Wu is expecting that she would because since day one he's telling me that she would be there to help you with you know like from Houston side she's not starting anywhere near I'm telling you because I have dealt with USCIS nobody has dealt better than me all these visa things yeah I know I really don't think this is working these are things yeah i i really don't think this is working now you're writing to him oh it's your h1b uh no no in here to houston stuff h1B yeah alright guys back you know I said for long it's just yes I've again become that limping astha that I was two years back just remember to take all the vitamin. Huh? Vitamin. Once it starts, it's like it takes so life to you. My entire my ankle, I just feel numb. I just can't feel it. It's so bad. That's why sometimes it feels shaky. Like I feel like I'll fall down. Really? My nerve gets compressed. Some nerve gets compressed. Some nerve gets compressed. So because of recently so much... You don't do too much yoga at this point. No, I'm not doing. I can't do anything now. I just need to... But I just do some back exercise to reduce the compression. But because I've been... I don't sit for so long. But these days I've been literally sitting for like 8-8 days. I've been I don't sit for so long but these days I've been literally sitting for like 8 days I've gained so much weight just because I'm sitting in one place I can't move then my problem will start see my foot I cannot put my complete foot on the floor I'm going back take care Shania if you need anything message me okay thank you. Janelle, if you need anything, message me. Okay, thank you. I heard...... I heard... hmm... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... I heard... Nettokens-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-bottom. Nettokens i-b kg of chicken breast a day. 1 kg of chicken breast a day. 1 kg of chicken breast a day. 1 kg of chicken breast a day. 1 kg of chicken breast a day. 1 kg of chicken breast a day. 1 kg put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will put it in the refrigerator. I will use the same method for the other side.ертвы. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side. I will use the same method for the other side 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 52, 53, 52, 53, 54, 55, 56, 57, 57, 58, 58 Takk for watching!\n"
          ]
        }
      ]
    }
  ]
}